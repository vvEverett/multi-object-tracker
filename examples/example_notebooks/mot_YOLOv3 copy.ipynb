{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple object tracking with YOLOv3-based object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from motrackers.detectors import Nanodet\n",
    "from motrackers import CentroidTracker, CentroidKF_Tracker, SORT, IOUTracker\n",
    "from motrackers.utils import draw_tracks\n",
    "from nanodet.util import Logger, cfg, load_config, load_model_weight\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = r\"D:\\shijue\\LiquidDrop\\22.avi\"\n",
    "WEIGHTS_PATH = r'D:\\shijue\\multi-object-tracker\\weight\\LiquidV4.pth'\n",
    "CONFIG_FILE_PATH = r'D:\\shijue\\multi-object-tracker\\config\\LiquidDetect416.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e2f69a12a44156811e0421ab0e949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='MOTracker:', options=('CentroidTracker', 'CentroidKF_Tracker', 'SORT', 'IOUTracker'), valu…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_tracker = widgets.Select(\n",
    "    options=[\"CentroidTracker\", \"CentroidKF_Tracker\", \"SORT\", \"IOUTracker\"],\n",
    "    value='CentroidTracker',\n",
    "    rows=5,\n",
    "    description='MOTracker:',\n",
    "    disabled=False\n",
    ")\n",
    "chosen_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chosen_tracker.value == 'CentroidTracker':\n",
    "    tracker = CentroidTracker(max_lost=0, tracker_output_format='mot_challenge')\n",
    "elif chosen_tracker.value == 'CentroidKF_Tracker':\n",
    "    tracker = CentroidKF_Tracker(max_lost=0, tracker_output_format='mot_challenge')\n",
    "elif chosen_tracker.value == 'SORT':\n",
    "    tracker = SORT(max_lost=3, tracker_output_format='mot_challenge', iou_threshold=0.3)\n",
    "elif chosen_tracker.value == 'IOUTracker':\n",
    "    tracker = IOUTracker(max_lost=2, iou_threshold=0.5, min_detection_confidence=0.4, max_detection_confidence=0.7,\n",
    "                         tracker_output_format='mot_challenge')\n",
    "else:\n",
    "    print(\"Please choose one tracker from the above list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size is  1.0x\n",
      "init weights...\n",
      "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\n",
      "Finish initialize NanoDet-Plus Head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m[root]\u001b[0m\u001b[34m[04-10 21:59:08]\u001b[0m\u001b[32mINFO:\u001b[0m\u001b[37mPress \"Esc\", \"q\" or \"Q\" to exit.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 导入模型文件\n",
    "local_rank = 0\n",
    "modelpath = WEIGHTS_PATH\n",
    "device = \"cpu:0\"\n",
    "config = CONFIG_FILE_PATH\n",
    "logger = Logger(local_rank, use_tensorboard=False)\n",
    "load_config(cfg, config)\n",
    "detmodel = Nanodet(cfg, modelpath, logger, device)\n",
    "logger.log('Press \"Esc\", \"q\" or \"Q\" to exit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(video_path, model, tracker):\n",
    "\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    while True:\n",
    "        ok, image = cap.read()\n",
    "\n",
    "        if not ok:\n",
    "            print(\"Cannot read the video feed.\")\n",
    "            break\n",
    "        \n",
    "        meta, res = model.inference(image)\n",
    "        bboxes,confidences,class_ids,updated_image  = model.visualize(res[0], meta, cfg.class_names, 0.43)\n",
    "        \n",
    "        tracks = tracker.update(bboxes, confidences, class_ids)\n",
    "\n",
    "        updated_image = draw_tracks(updated_image, tracks)\n",
    "\n",
    "        cv.imshow(\"image\", updated_image)\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward time: 0.156s | decode time: 0.038s | viz time: 0.003s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\ist\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main(VIDEO_FILE, detmodel, tracker)\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(video_path, model, tracker)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     11\u001b[0m meta, res \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minference(image)\n\u001b[1;32m---> 12\u001b[0m bboxes,confidences,class_ids,updated_image  \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mvisualize(res[\u001b[39m0\u001b[39m], meta, cfg\u001b[39m.\u001b[39mclass_names, \u001b[39m0.43\u001b[39m)\n\u001b[0;32m     14\u001b[0m tracks \u001b[39m=\u001b[39m tracker\u001b[39m.\u001b[39mupdate(bboxes, confidences, class_ids)\n\u001b[0;32m     16\u001b[0m updated_image \u001b[39m=\u001b[39m draw_tracks(updated_image, tracks)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "main(VIDEO_FILE, detmodel, tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
